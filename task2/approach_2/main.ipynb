{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT EMBEDDINGS CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_CLUSTERS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.cluster import KMeans\n",
    "import joblib\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import pos_tag, word_tokenize, ne_chunk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')  # for tokenization\n",
    "nltk.download('averaged_perceptron_tagger')  # for POS-tagging\n",
    "nltk.download('maxent_ne_chunker')  # for NER\n",
    "nltk.download('words')  # for NER\n",
    "nltk.download('stopwords')  # for NER\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# get media description\n",
    "\n",
    "texts = np.load('data/tweet_desc.npy')\n",
    "\n",
    "def train_and_save_cluster():\n",
    "    # Step 2: Apply KMeans clustering to obtain cluster assignments\n",
    "    try:\n",
    "        embeddings_array = np.load('data/media_bert_embeddings_array.npy') # load if already saved\n",
    "    except Exception as e:\n",
    "        embeddings_list = [model(tokenizer.encode(text, return_tensors='pt'))[0][:, 0, :].detach().numpy() for text in texts]\n",
    "        embeddings_array = torch.cat([torch.from_numpy(embeddings) for embeddings in embeddings_list]).numpy()\n",
    "\n",
    "    num_clusters = NUMBER_OF_CLUSTERS  # Adjust based on your optimal number of clusters -> maybe 10\n",
    "    kmeans = KMeans(n_clusters=num_clusters, n_init=10, random_state=42)\n",
    "    bert_clusters = kmeans.fit_predict(embeddings_array)\n",
    "\n",
    "    joblib.dump(kmeans, 'saved/kmeans_bert_model.pkl')  # save model\n",
    "    np.save('saved/bert_clusters.npy', bert_clusters)  # save clusters\n",
    "\n",
    "    return kmeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_keywords(texts, clusters):\n",
    "\n",
    "    results = list(zip(texts, clusters))\n",
    "    cluster_keywords = [[] for i in range(NUMBER_OF_CLUSTERS)]\n",
    "\n",
    "    for text, cluster_id in results:\n",
    "\n",
    "        # Tokenize and POS-tag the text\n",
    "        tokens = word_tokenize(text)\n",
    "        pos_tags = pos_tag(tokens)\n",
    "\n",
    "        # Apply NER using NLTK\n",
    "        tree = ne_chunk(pos_tags)\n",
    "        named_entities = [chunk.label() for chunk in tree if hasattr(chunk, 'label')]\n",
    "        \n",
    "        # Extract action verbs\n",
    "        action_verbs = [word for (word, pos) in pos_tags if pos.startswith(\"VB\")]\n",
    "\n",
    "        named_entities = [entity.lower() for entity in named_entities]\n",
    "\n",
    "        keywords = named_entities + action_verbs\n",
    "\n",
    "        for keyword in keywords:\n",
    "            cluster_keywords[cluster_id].append(keyword)\n",
    "            \n",
    "    return cluster_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = train_and_save_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_from_media(kmeans, media_desc:str):\n",
    "    embedding = model(tokenizer.encode(media_desc, return_tensors='pt'))[0][:, 0, :].detach().numpy()\n",
    "    current_cluster_id = kmeans.predict(embedding)[0]\n",
    "\n",
    "    clusters = np.load('saved/bert_clusters.npy')\n",
    "    cluster_keywords = get_cluster_keywords(texts, clusters)\n",
    "    keywords = cluster_keywords[current_cluster_id]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COSINE SIMILARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.vectorizers import CountVectorizer\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_cosine_similarity(word1, word2): # using bert embeddings\n",
    "    # Use BERT to find embeddings for each word\n",
    "    # word1_embedding = model(tokenizer.encode(word1, return_tensors='pt'))[0][:,0,:].detach().numpy()\n",
    "    # word2_embedding = model(tokenizer.encode(word2, return_tensors='pt'))[0][:,0,:].detach().numpy()\n",
    "\n",
    "    vectorizer = CountVectorizer().fit_transform([word1, word2])\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity_matrix = cosine_similarity(vectorizer)\n",
    "\n",
    "    # Return the cosine similarity score\n",
    "    return similarity_matrix[0, 0]\n",
    "\n",
    "def get_top_k_similar_words(list1, list2, k=1):\n",
    "    # Initialize a dictionary to store similarity scores\n",
    "    similarity_scores = {}\n",
    "\n",
    "    # Iterate through each pair of words in the two lists\n",
    "    for word1 in list1:\n",
    "        for word2 in list2:\n",
    "            # Calculate cosine similarity\n",
    "            similarity = get_cosine_similarity(word1, word2)\n",
    "            \n",
    "            # Store the similarity score in the dictionary\n",
    "            key = (word1, word2)\n",
    "            similarity_scores[key] = similarity\n",
    "\n",
    "    # Sort the similarity scores in descending order\n",
    "    sorted_scores = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top-k similar pairs\n",
    "    top_k_pairs = sorted_scores[:k]\n",
    "\n",
    "    return top_k_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with two lists of keywords\n",
    "keywords_list1 = [\"women\", \"blue shirt\", \"vests\", \"working\", \"working in factory\", \"factory\"]\n",
    "keywords_list2 = [\"empowerment\", \"blue shirts\", \"workforce\", \"factory\", \"film production\"]\n",
    "\n",
    "# Specify the value of k (top-k similar pairs)\n",
    "top_k = 6\n",
    "\n",
    "# Get the top-k similar pairs\n",
    "top_k_pairs = get_top_k_similar_words(keywords_list1, keywords_list2, k=top_k)\n",
    "\n",
    "# Print the top-k similar pairs and their scores\n",
    "for pair, score in top_k_pairs:\n",
    "    word1, word2 = pair\n",
    "    print(f\"Similarity between '{word1}' and '{word2}': {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIKE MAPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_from_likes(likes):\n",
    "\n",
    "    likes_keyword_mapping = pd.read_csv('/content/drive/My Drive/Tech-Mid-Adobe/task2/likes_keywords_mapping.csv')\n",
    "    closest_index = (likes_keyword_mapping['likes'] - likes).abs().idxmin()\n",
    "\n",
    "    return eval(likes_keyword_mapping.loc[closest_index]['keywords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the results from this approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As the social media manager for 'CNN' (Twitter: @CNN), create a tweet using the following keywords: ['hey', 'mellow', 'tello']. Craft a message that aligns with our brand and is likely to receive at least 100 likes.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keywords = ['hey', 'mellow', 'tello']\n",
    "# company = 'CNN'\n",
    "# username = 'CNN'\n",
    "# like = 100\n",
    "# prompt_given_company = f\"You are the social-media manager of company '{company}' having twitter username '{username}' and you have the following keywords {str(k)}, you have write a tweet in the same format as the previous tweets of your company using the given keywords  so that it gets atleast {like} likes\"\n",
    "\n",
    "prompt_given_company = \"As the social media manager for '{company}' (Twitter: @{username}), create a tweet using the following keywords: {keywords}. Craft a message that aligns with our brand and is likely to receive at least {like} likes.\"\n",
    "\n",
    "prompt_given_company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "like_mappings = []\n",
    "keywords = get_keywords_from_media(kmeans, media_desc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
