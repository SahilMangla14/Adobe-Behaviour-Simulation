{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpl0M1jtiuG-"
      },
      "source": [
        "BERT EMBEDDINGS CLUSTERING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46ppEP7DiuHD"
      },
      "outputs": [],
      "source": [
        "NUMBER_OF_CLUSTERS = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install transformers\n",
        "!pip install nltk\n",
        "!pip install sklearn\n",
        "!pip install numpy\n",
        "!pip install joblib\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install spacy\n",
        "!pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install --upgrade -q google-api-python-client google-auth-httplib2 google-auth-oauthlib\n",
        "!pip install google-generativeai\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHfiUICliuHH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.cluster import KMeans\n",
        "import joblib\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk import pos_tag, word_tokenize, ne_chunk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')  # for tokenization\n",
        "nltk.download('averaged_perceptron_tagger')  # for POS-tagging\n",
        "nltk.download('maxent_ne_chunker')  # for NER\n",
        "nltk.download('words')  # for NER\n",
        "nltk.download('stopwords')  # for NER\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# get media description\n",
        "# for company\n",
        "texts = np.load('../data/test_company_tweet_desc.npy', allow_pickle=True)\n",
        "\n",
        "# for time\n",
        "# texts = np.load('../data/test_time_tweet_desc.npy', allow_pickle=True)\n",
        "\n",
        "def train_and_save_cluster():\n",
        "    # Step 2: Apply KMeans clustering to obtain cluster assignments\n",
        "    try:\n",
        "        embeddings_array = np.load('../data/media_bert_embeddings_array.npy') # load if already saved\n",
        "    except Exception as e:\n",
        "        embeddings_list = [model(tokenizer.encode(text, return_tensors='pt'))[0][:, 0, :].detach().numpy() for text in texts]\n",
        "        embeddings_array = torch.cat([torch.from_numpy(embeddings) for embeddings in embeddings_list]).numpy()\n",
        "\n",
        "    num_clusters = NUMBER_OF_CLUSTERS  # Adjust based on your optimal number of clusters -> maybe 10\n",
        "    kmeans = KMeans(n_clusters=num_clusters, n_init=10, random_state=42)\n",
        "    bert_clusters = kmeans.fit_predict(embeddings_array)\n",
        "\n",
        "    joblib.dump(kmeans, '../saved/kmeans_bert_model.pkl')  # save model\n",
        "    np.save('../saved/bert_clusters.npy', bert_clusters)  # save clusters\n",
        "\n",
        "    return kmeans\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_caDhI6ItdAB"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def extract_keywords(text):\n",
        "    # Process the text using spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Filter out stop words and non-content words\n",
        "    keywords = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
        "    verbs = [token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
        "\n",
        "    return keywords+verbs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JgnA277iuHI"
      },
      "outputs": [],
      "source": [
        "def get_cluster_keywords(texts, clusters):\n",
        "\n",
        "    results = list(zip(texts, clusters))\n",
        "    cluster_keywords = [[] for _ in range(NUMBER_OF_CLUSTERS)]\n",
        "\n",
        "    for text, cluster_id in results:\n",
        "        # Tokenize and POS-tag the text\n",
        "        tokens = word_tokenize(text['media'])\n",
        "        pos_tags = pos_tag(tokens)\n",
        "\n",
        "        # Apply NER using NLTK\n",
        "        tree = ne_chunk(pos_tags)\n",
        "        named_entities = [chunk.label() for chunk in tree if hasattr(chunk, 'label')]\n",
        "\n",
        "        # Extract action verbs\n",
        "        action_verbs = [word for (word, pos) in pos_tags if pos.startswith(\"VB\")]\n",
        "\n",
        "        named_entities = [entity.lower() for entity in named_entities]\n",
        "\n",
        "        keywords = named_entities+action_verbs\n",
        "        # print(keywords)\n",
        "        # keywords = extract_keywords(text['media'])\n",
        "        for keyword in keywords:\n",
        "            # print(\"keyword \", keyword)\n",
        "            cluster_keywords[cluster_id].append(keyword)\n",
        "            # np.append(cluster_keywords[cluster_id], keyword)\n",
        "        # break\n",
        "\n",
        "    return cluster_keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_lbIja7iuHJ"
      },
      "outputs": [],
      "source": [
        "kmeans = train_and_save_cluster()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWTLzsW6iuHK"
      },
      "outputs": [],
      "source": [
        "def get_keywords_from_media(kmeans, media_desc:str):\n",
        "\n",
        "      embedding = model(tokenizer.encode(media_desc, return_tensors='pt'))[0][:, 0, :].detach().numpy()\n",
        "      current_cluster_id = kmeans.predict(embedding)[0]\n",
        "\n",
        "      clusters = np.load('../saved/bert_clusters.npy')\n",
        "      cluster_keywords = get_cluster_keywords(texts, clusters)\n",
        "\n",
        "      keywords = cluster_keywords[current_cluster_id]\n",
        "      return keywords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5LAgV0-iuHN"
      },
      "source": [
        "LIKE MAPPING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eExFMrXQiuHO"
      },
      "outputs": [],
      "source": [
        "def get_keywords_from_likes(likes):\n",
        "\n",
        "    likes_keyword_mapping = pd.read_csv('../data/likes_keywords_mapping.csv')\n",
        "    closest_index = (likes_keyword_mapping['likes'] - likes).abs().idxmin()\n",
        "\n",
        "    return eval(likes_keyword_mapping.loc[closest_index]['keywords'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV8Jn3mBiuHP"
      },
      "source": [
        "Check the results from this approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "9_-yKxEBiuHP",
        "outputId": "0c957dfb-db7c-476d-f905-6aa040a69be2"
      },
      "outputs": [],
      "source": [
        "# keywords = ['hey', 'mellow', 'tello']\n",
        "text = texts[0]\n",
        "company = text['company']\n",
        "username = text['username']\n",
        "like = 100\n",
        "\n",
        "like_mappings = get_keywords_from_likes(like)\n",
        "keywords = get_keywords_from_media(kmeans, text['media'])\n",
        "\n",
        "# prompt_given_company = f\"You are the social-media manager of company '{company}' having twitter username '{username}' and you have the following keywords {str(k)}, you have write a tweet in the same format as the previous tweets of your company using the given keywords  so that it gets atleast {like} likes\"\n",
        "\n",
        "prompt_given_company = f\"As the social media manager for '{company}' (Twitter: @{username}), create a tweet using the following keywords: {str(keywords)}. Craft a message that aligns with our brand and is likely to receive at least {like} likes.\"\n",
        "\n",
        "prompt_given_company"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNSFqrrEiuHM"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# model = BertModel.rom_pretrained('bert-base-uncased')\n",
        "\n",
        "def get_word2vec_model(sentences):\n",
        "  return Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "model = get_word2vec_model([keywords, like_mappings])\n",
        "\n",
        "def get_cosine_similarity(word1, word2): # using bert embeddings\n",
        "    # Use BERT to find embeddings for each word\n",
        "    # word1_embedding = model(tokenizer.encode(word1, return_tensors='pt'))[0][:,0,:].detach().numpy()\n",
        "    # word2_embedding = model(tokenizer.encode(word2, return_tensors='pt'))[0][:,0,:].detach().numpy()\n",
        "\n",
        "    # vectorizer = CountVectorizer().fit_transform([word1, word2])\n",
        "    try:\n",
        "        if word1.lower()==word2.lower(): # same words, similarity is 1\n",
        "          return 1.0\n",
        "\n",
        "        vec1 = model.wv[word1]\n",
        "        vec2 = model.wv[word2]\n",
        "        similarity = cosine_similarity([vec1], [vec2])[0, 0]\n",
        "        return similarity\n",
        "    except KeyError:\n",
        "        return 0.0  # Return 0 if either word is not in the vocabulary\n",
        "\n",
        "def get_top_k_similar_words(list1, list2, k=1):\n",
        "    # Create matrices of all pairwise similarities\n",
        "    matrix = np.zeros((len(list1), len(list2)))\n",
        "    for i, word1 in enumerate(list1):\n",
        "        for j, word2 in enumerate(list2):\n",
        "            matrix[i, j] = get_cosine_similarity(word1, word2)\n",
        "\n",
        "    # Get the indices of the top-k similarities\n",
        "    indices = np.argpartition(matrix, -k, axis=None)[-k:]\n",
        "    top_k_indices = np.unravel_index(indices, matrix.shape)\n",
        "\n",
        "    # Extract the top-k pairs and their similarities\n",
        "    top_k_pairs = [(list1[i], list2[j], matrix[i, j]) for i, j in zip(*top_k_indices)]\n",
        "\n",
        "    # Filter to include only pairs with similarity score less than 1.0\n",
        "    top_k_pairs = [(word1, word2, similarity) for word1, word2, similarity in top_k_pairs if similarity < 1.00]\n",
        "\n",
        "    return top_k_pairs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TESTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sx9-ikfQFQSP"
      },
      "outputs": [],
      "source": [
        "# Install the client library and import necessary modules.\n",
        "# !pip install google-generativeai\n",
        "import google.generativeai as palm\n",
        "import base64\n",
        "import json\n",
        "import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os.path\n",
        "\n",
        "from google.auth.transport.requests import Request\n",
        "from google.oauth2.credentials import Credentials\n",
        "from google_auth_oauthlib.flow import InstalledAppFlow\n",
        "\n",
        "SCOPES = ['https://www.googleapis.com/auth/generative-language.tuning']\n",
        "\n",
        "def load_creds():\n",
        "    \"\"\"Converts `oauth-client-id.json` to a credential object.\n",
        "\n",
        "    This function caches the generated tokens to minimize the use of the\n",
        "    consent screen.\n",
        "    \"\"\"\n",
        "    creds = None\n",
        "    # The file token.json stores the user's access and refresh tokens, and is\n",
        "    # created automatically when the authorization flow completes for the first\n",
        "    # time.\n",
        "    if os.path.exists('token.json'):\n",
        "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
        "    # If there are no (valid) credentials available, let the user log in.\n",
        "    if not creds or not creds.valid:\n",
        "        if creds and creds.expired and creds.refresh_token:\n",
        "            creds.refresh(Request())\n",
        "        else:\n",
        "            flow = InstalledAppFlow.from_client_secrets_file(\n",
        "                'oauth-client-id.json', SCOPES)\n",
        "            creds = flow.run_local_server(port=0)\n",
        "        # Save the credentials for the next run\n",
        "        with open('token.json', 'w') as token:\n",
        "            token.write(creds.to_json())\n",
        "    return creds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pprint\n",
        "import google.generativeai as palm\n",
        "\n",
        "creds = load_creds()\n",
        "\n",
        "palm.configure(credentials=creds)\n",
        "\n",
        "print('Available base models:', [m.name for m in palm.list_models()])\n",
        "print('My tuned models:', [m.name for m in palm.list_tuned_models()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = 'models/text-bison-001'\n",
        "model = palm.get_model(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = model_name \n",
        "temperature = 0 \n",
        "candidate_count = 1 \n",
        "top_k = 40 \n",
        "top_p = 0.95 \n",
        "max_output_tokens = 1024 \n",
        "\n",
        "defaults = {\n",
        "    'model': model,\n",
        "    'temperature': temperature,\n",
        "    'candidate_count': candidate_count,\n",
        "    'top_k': top_k,\n",
        "    'top_p': top_p,\n",
        "    'max_output_tokens': max_output_tokens,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_response(prompt, defaults):\n",
        "    \"\"\"Returns the response from the model.\"\"\"\n",
        "    response = palm.generate_text(**defaults,prompt=prompt)\n",
        "    \n",
        "    if len(response.candidates) == 0:\n",
        "        return ' '\n",
        "    return response.candidates[0]['output']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_template = \"As the social media manager for '{company}' (Twitter: @{username}), create a tweet using the following keywords: {keywords}. Craft a message that aligns with our brand and is likely to receive at least {like} likes.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generated_and_actual[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "test_tweet = np.load('../data/test_company_tweet_desc.npy', allow_pickle=True)\n",
        "\n",
        "def run_test(test_type='time'):\n",
        "    generated_and_actual = []\n",
        "\n",
        "    for i in range( len(test_tweet) ):\n",
        "        try:\n",
        "            tweet = test_tweet[i]\n",
        "            like = tweet['likes']\n",
        "            company = tweet['company']\n",
        "            date = tweet['date']\n",
        "            media_desc = tweet['media']\n",
        "            keywords = extract_keywords(media_desc)\n",
        "            username = tweet['username']\n",
        "            like_mappings = get_keywords_from_likes(like)\n",
        "\n",
        "            top_k = get_top_k_similar_words(keywords, like_mappings, k=10)\n",
        "\n",
        "            keywords = keywords + [ tup[1] for tup in top_k ]\n",
        "            prompt = prompt_template.format(company=company, date=date, like=like, keywords=str(keywords), username=username)\n",
        "            generated_response = get_response(prompt, defaults)\n",
        "\n",
        "            generated_and_actual.append({\n",
        "                'id': tweet['id'],\n",
        "                'generated': generated_response\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}. index: {i}\")\n",
        "            \n",
        "    df = pd.DataFrame(generated_and_actual)\n",
        "    df.to_csv(f'../saved/test_{test_type}_final_submit_result.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_test('company')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
